{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kpYwy7woFX8l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Indonesian-English-Bilingual-Corpus-master.zip: 3.00MiB [00:01, 1.85MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished downloading\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import os\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from munch import Munch\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def download_file(url: str, path: Path):\n",
        "    resp = requests.get(url, stream=True)\n",
        "\n",
        "    content_disp = resp.headers['content-disposition']\n",
        "\n",
        "    if content_disp:\n",
        "        pattern = 'filename='\n",
        "        index = content_disp.find(pattern)\n",
        "\n",
        "        if index > -1:\n",
        "            name = content_disp[index+len(pattern):]\n",
        "        else:\n",
        "            name = url.split('/')[-1]\n",
        "    else:\n",
        "        name = url.split('/')[-1]\n",
        "\n",
        "    dest = path / name\n",
        "\n",
        "    tqdm_args = {\n",
        "        'desc': name,\n",
        "        'total': int(resp.headers.get('content-length', 0)),\n",
        "        'unit': 'iB',\n",
        "        'unit_scale': True,\n",
        "        'unit_divisor': 1024\n",
        "    }\n",
        "\n",
        "    with open(dest, 'wb') as file, tqdm(**tqdm_args) as bar:\n",
        "        for data in resp.iter_content(chunk_size=1024):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "    return dest\n",
        "\n",
        "def _parallel_dataset(path: Path, prefix: str='parallel'):\n",
        "    url = 'https://codeload.github.com/prasastoadi/parallel-corpora-en-id/zip/refs/heads/master'\n",
        "    dest = download_file(url, path)\n",
        "\n",
        "    with zipfile.ZipFile(dest, 'r') as zf:\n",
        "        for f in zf.namelist():\n",
        "            if f.endswith('.tgz'):\n",
        "\n",
        "                with zf.open(f, 'r') as fo, tarfile.open(fileobj=fo, mode='r') as tf:\n",
        "                    for m in tf.getmembers():\n",
        "                        if m.name.endswith('.en') or '-EN-' in m.name:\n",
        "                            tf.extract(m, path=path / prefix /'en')\n",
        "                        else:\n",
        "                            tf.extract(m, path=path / prefix / 'id')\n",
        "\n",
        "    dest.unlink()\n",
        "    \n",
        "def _bilingual_dataset(path: Path, prefix: str='bilingual'):\n",
        "    url = 'https://github.com/desmond86/Indonesian-English-Bilingual-Corpus/archive/refs/heads/master.zip'\n",
        "    dest = download_file(url, path)\n",
        "\n",
        "    with zipfile.ZipFile(dest, 'r') as zf:\n",
        "        for f in zf.namelist():\n",
        "            en = f.endswith('.en')\n",
        "            id = f.endswith('.id')\n",
        "\n",
        "            if en or id:\n",
        "                dest_path = path / prefix / ('en' if en else 'id')\n",
        "                dest_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with zf.open(f, 'r') as fo, open(dest_path / os.path.basename(f), 'wb') as t:\n",
        "                    t.write(fo.read())\n",
        "\n",
        "    dest.unlink()\n",
        "    \n",
        "def _talpco_dataset(path: Path, prefix: str='talpco'):\n",
        "    url = 'https://github.com/matbahasa/TALPCo/archive/refs/heads/master.zip'\n",
        "    dest = download_file(url, path)\n",
        "\n",
        "    path_en = path / prefix / 'en'\n",
        "    path_id = path / prefix / 'id'\n",
        "\n",
        "    path_en.mkdir(parents=True, exist_ok=True)\n",
        "    path_id.mkdir(exist_ok=True)\n",
        "\n",
        "    file_to_path = [\n",
        "        ('data_eng.txt', path_en),\n",
        "        ('data_ind.txt', path_id)\n",
        "    ]\n",
        "\n",
        "    with zipfile.ZipFile(dest, 'r') as zf:\n",
        "        for f in zf.namelist():\n",
        "            for fn, p in file_to_path:\n",
        "                if f.endswith(fn):\n",
        "                    \n",
        "                    with zf.open(f, 'r') as fo, open(p / fn, 'w') as t:\n",
        "                        items = []\n",
        "                        for line in fo.readlines():\n",
        "                            split = line.decode('UTF-8').split('\\t')\n",
        "            \n",
        "                            if len(split) > 1:\n",
        "                                items.append(split[1].replace('\\r', ''))\n",
        "                        \n",
        "                        t.writelines(items)\n",
        "                        \n",
        "    dest.unlink()\n",
        "    \n",
        "def download_all(path: str='raw_data'):\n",
        "    path = Path(path)\n",
        "    path.mkdir(exist_ok=True)\n",
        "    \n",
        "    print('Downloading data')\n",
        "\n",
        "    # _parallel_dataset(path)\n",
        "    _bilingual_dataset(path)\n",
        "    # _talpco_dataset(path)\n",
        "\n",
        "    print('Finished downloading')\n",
        "\n",
        "download_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building dataset\n",
            "Finished building dataset\n"
          ]
        }
      ],
      "source": [
        "def build_dataset(path='data',\n",
        "                    data_path='raw_data',\n",
        "                    langs=['en', 'id'],\n",
        "                    test_size=0.2,\n",
        "                    random_state=42):\n",
        "    path = Path(path)        \n",
        "    data_path = Path(data_path)\n",
        "\n",
        "    if not data_path.exists():\n",
        "        raise RuntimeError('No data is found, please download dataset first')\n",
        "\n",
        "    path.mkdir(exist_ok=True)\n",
        "\n",
        "    data = dict()\n",
        "    last_len = -1\n",
        "\n",
        "    print('Building dataset')\n",
        "\n",
        "    for lang in langs:\n",
        "        sent = []\n",
        "        \n",
        "        for name in data_path.iterdir():\n",
        "            for d in (name / lang).iterdir():\n",
        "                if d.is_file():\n",
        "                    with open(d, 'r', encoding='UTF-8') as file:\n",
        "                        # All sentences are already separated by a newline\n",
        "                        for line in file.readlines():\n",
        "                            sent.append(line.lower())\n",
        "\n",
        "        data[lang] = np.array(sent)\n",
        "\n",
        "        if last_len == -1:\n",
        "            last_len = len(sent)\n",
        "        elif last_len != len(sent):\n",
        "            raise RuntimeError('Data length of sentence mismatch ({} -> {} ({}))'.format(last_len, len(sent), lang))\n",
        "\n",
        "\n",
        "    state = np.random.get_state()\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    msk = np.random.rand(last_len) > test_size\n",
        "    np.random.set_state(state)\n",
        "    \n",
        "    for lang in langs:\n",
        "        with open(path / (lang + '_train.txt'), 'w', encoding='UTF-8') as file:\n",
        "            file.writelines(data[lang][msk])\n",
        "            \n",
        "        with open(path / (lang + '_test.txt'), 'w', encoding='UTF-8') as file:\n",
        "            file.writelines(data[lang][~msk])\n",
        "\n",
        "    print('Finished building dataset')\n",
        "\n",
        "build_dataset(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import math\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, max_length: int=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pos = torch.arange(0, max_length).reshape(max_length, 1)\n",
        "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "\n",
        "        pos_emb = torch.zeros((max_length, emb_size))\n",
        "        pos_emb[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_emb[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_emb = pos_emb.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_emb', pos_emb)\n",
        "\n",
        "    def forward(self, token_emb):\n",
        "        return self.dropout(token_emb + self.pos_emb[:token_emb.size(0), :])\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.emb(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                encoder_layers: int,\n",
        "                decode_layers: int,\n",
        "                emb_size: int,\n",
        "                nhead: int,\n",
        "                src_vocab_size: int,\n",
        "                tgt_vocab_size: int,\n",
        "                dim_feedforward: int,\n",
        "                dropout: float):\n",
        "                \n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "        self.src_token_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_token_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        \n",
        "        self.transformer = nn.Transformer(d_model=emb_size,\n",
        "                                          nhead=nhead,\n",
        "                                          num_encoder_layers=encoder_layers,\n",
        "                                          num_decoder_layers=decode_layers,\n",
        "                                          dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout)\n",
        "\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "    \n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                tgt: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_pad_mask: Tensor,\n",
        "                tgt_pad_mask: Tensor,\n",
        "                mem_key_pad_mask: Tensor):\n",
        "\n",
        "        src_emb = self.pos_encoding(self.src_token_emb(src))\n",
        "        tgt_emb = self.pos_encoding(self.tgt_token_emb(tgt))\n",
        "\n",
        "        out_seq = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                   src_pad_mask, tgt_pad_mask,\n",
        "                                   mem_key_pad_mask)\n",
        "\n",
        "        return self.generator(out_seq)\n",
        "        \n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.pos_encoding(self.src_token_emb(src)), src_mask)\n",
        "    \n",
        "    def decode(self, tgt: Tensor, mem: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.pos_encoding(self.tgt_token_emb(tgt)), mem, tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(size):\n",
        "    mask = (torch.triu(torch.ones((size, size), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, -1e9).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
        "\n",
        "    src_pad_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_pad_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "\n",
        "    return src_mask, tgt_mask, src_pad_mask, tgt_pad_mask\n",
        "\n",
        "def train_epoch(model, opt, loss_fn, train_loader):\n",
        "    model.train()\n",
        "    losses = 0.0\n",
        "\n",
        "    for src, tgt in train_loader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        opt.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_loader)\n",
        "\n",
        "def evaluate(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    losses = 0.0\n",
        "\n",
        "    for src, tgt in val_loader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing data (en/train): 100%|██████████| 32339/32339 [00:05<00:00, 5436.89it/s]\n",
            "Tokenizing data (en/test): 100%|██████████| 8030/8030 [00:01<00:00, 7513.47it/s]\n",
            "Tokenizing data (id/train): 100%|██████████| 32339/32339 [00:06<00:00, 5052.52it/s]\n",
            "Tokenizing data (id/test): 100%|██████████| 8030/8030 [00:01<00:00, 7319.48it/s]\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "specials = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
        "\n",
        "def token_transform(tokenizer, sent):\n",
        "    tokens = []\n",
        "    for tok in tokenizer(sent):\n",
        "        tok = str(tok)\n",
        "\n",
        "        if any(c.isdigit() for c in tok):\n",
        "            continue\n",
        "\n",
        "        if all(c in string.punctuation for c in tok):\n",
        "            continue\n",
        "\n",
        "        tokens.append(tok)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def vocab_transform(vocab, sent_tok):\n",
        "    return [SOS_IDX] + \\\n",
        "           [vocab[tok] if tok in vocab.keys() else UNK_IDX for tok in sent_tok] + \\\n",
        "           [EOS_IDX]\n",
        "\n",
        "def load_dataset(path: str='data',\n",
        "                 src_lang: str='en',\n",
        "                 tgt_lang: str='id'):\n",
        "    path = Path(path)\n",
        "\n",
        "    if not path.exists():\n",
        "        raise RuntimeError('No data is found, please download and/or build dataset first')\n",
        "\n",
        "    data = dict()\n",
        "\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        token_ids = dict()\n",
        "        vocab = dict()\n",
        "\n",
        "        try:\n",
        "            tokenizer = spacy.load(lang)\n",
        "        except:\n",
        "            tokenizer = spacy.blank(lang)\n",
        "\n",
        "        for type in ['train', 'test']:\n",
        "            sentences = []\n",
        "\n",
        "            with open(path / (lang + '_' + type + '.txt'), 'r', encoding='UTF-8') as file:\n",
        "                for line in file.readlines():\n",
        "                    sentences.append(line.lower().rstrip('\\n'))\n",
        "\n",
        "            sent_tokens = []\n",
        "\n",
        "            for sent in tqdm(sentences, f'Tokenizing data ({lang}/{type})'):\n",
        "                sent_tokens.append(token_transform(tokenizer, sent))\n",
        "\n",
        "            if type == 'train':\n",
        "                counter = Counter()\n",
        "                for tokens in sent_tokens:\n",
        "                    counter.update(tokens)\n",
        "\n",
        "                for tok in specials:\n",
        "                    del counter[tok]\n",
        "\n",
        "                sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[0])\n",
        "                sorted_by_freq_tuples.sort(key=lambda x: x[1], reverse=True)\n",
        "                ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "                for symbol in specials[::-1]:\n",
        "                    ordered_dict.update({symbol: 1})\n",
        "                    ordered_dict.move_to_end(symbol, last=False)\n",
        "\n",
        "                vocab = dict(zip(ordered_dict, range(len(ordered_dict))))\n",
        "\n",
        "            token_ids[type] = []\n",
        "            for sent_tok in sent_tokens:\n",
        "                token_ids[type].append(vocab_transform(vocab, sent_tok))\n",
        "\n",
        "        data[lang] = Munch(vocab=vocab, data=token_ids, tokenizer=tokenizer)\n",
        "\n",
        "    return data\n",
        "\n",
        "dataset = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "for lang in ['en', 'id']:\n",
        "    with open(lang + '_vocab.txt', 'w', encoding='UTF-8') as f:\n",
        "        f.writelines('\\n'.join(dataset[lang].vocab.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn8tWAAPGWBA",
        "outputId": "79a5d646-a02e-4084-8821-ba321ea34d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src vocab size: 18727, tgt vocab size: 18586\n"
          ]
        }
      ],
      "source": [
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "src_lang = 'en'\n",
        "tgt_lang = 'id'\n",
        "\n",
        "train_batch_size = 40\n",
        "val_batch_size = 40\n",
        "\n",
        "class SequenceDataset(data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def get_pair_set(data, type):\n",
        "    return [(x, y) for x, y in zip(data[src_lang].data[type], data[tgt_lang].data[type])]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(torch.tensor(src_sample))\n",
        "        tgt_batch.append(torch.tensor(tgt_sample))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    \n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "train_loader = data.DataLoader(dataset=SequenceDataset(get_pair_set(dataset, 'train')),\n",
        "                        batch_size=train_batch_size,\n",
        "                        collate_fn=collate_fn)\n",
        "\n",
        "val_loader = data.DataLoader(dataset=SequenceDataset(get_pair_set(dataset, 'test')),\n",
        "                        batch_size=val_batch_size,\n",
        "                        collate_fn=collate_fn)\n",
        "\n",
        "src_vocab_size = len(dataset[src_lang].vocab)\n",
        "tgt_vocab_size = len(dataset[tgt_lang].vocab)\n",
        "\n",
        "print('src vocab size: {}, tgt vocab size: {}'.format(src_vocab_size, tgt_vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9n5uLRafH41s"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Seq2SeqTransformer(3, 3, 256, 8,\n",
        "                           src_vocab_size, tgt_vocab_size,\n",
        "                           256, 0.1)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 7.326, Val loss: 7.071, Epoch time = 81.908s\n",
            "Epoch: 2, Train loss: 6.650, Val loss: 6.819, Epoch time = 75.558s\n",
            "Epoch: 3, Train loss: 6.292, Val loss: 6.580, Epoch time = 75.631s\n",
            "Epoch: 4, Train loss: 5.973, Val loss: 6.263, Epoch time = 76.119s\n",
            "Epoch: 5, Train loss: 5.682, Val loss: 6.005, Epoch time = 75.606s\n",
            "Epoch: 6, Train loss: 5.409, Val loss: 5.747, Epoch time = 75.652s\n",
            "Epoch: 7, Train loss: 5.157, Val loss: 5.491, Epoch time = 75.624s\n",
            "Epoch: 8, Train loss: 4.923, Val loss: 5.282, Epoch time = 75.688s\n",
            "Epoch: 9, Train loss: 4.712, Val loss: 5.101, Epoch time = 75.306s\n",
            "Epoch: 10, Train loss: 4.517, Val loss: 4.936, Epoch time = 89.099s\n",
            "Epoch: 11, Train loss: 4.338, Val loss: 4.788, Epoch time = 89.914s\n",
            "Epoch: 12, Train loss: 4.167, Val loss: 4.671, Epoch time = 89.906s\n",
            "Epoch: 13, Train loss: 4.013, Val loss: 4.555, Epoch time = 89.903s\n",
            "Epoch: 14, Train loss: 3.868, Val loss: 4.466, Epoch time = 89.909s\n",
            "Epoch: 15, Train loss: 3.733, Val loss: 4.390, Epoch time = 89.907s\n",
            "Epoch: 16, Train loss: 3.612, Val loss: 4.293, Epoch time = 89.974s\n",
            "Epoch: 17, Train loss: 3.495, Val loss: 4.240, Epoch time = 89.690s\n",
            "Epoch: 18, Train loss: 3.394, Val loss: 4.170, Epoch time = 89.686s\n",
            "Epoch: 19, Train loss: 3.291, Val loss: 4.123, Epoch time = 89.697s\n",
            "Epoch: 20, Train loss: 3.197, Val loss: 4.072, Epoch time = 89.681s\n",
            "Epoch: 21, Train loss: 3.106, Val loss: 4.055, Epoch time = 89.702s\n",
            "Epoch: 22, Train loss: 3.021, Val loss: 4.025, Epoch time = 89.688s\n",
            "Epoch: 23, Train loss: 2.942, Val loss: 4.003, Epoch time = 89.688s\n",
            "Epoch: 24, Train loss: 2.869, Val loss: 3.979, Epoch time = 89.681s\n",
            "Epoch: 25, Train loss: 2.798, Val loss: 3.968, Epoch time = 89.685s\n",
            "Epoch: 26, Train loss: 2.734, Val loss: 3.932, Epoch time = 89.713s\n",
            "Epoch: 27, Train loss: 2.671, Val loss: 3.934, Epoch time = 89.686s\n",
            "Epoch: 28, Train loss: 2.610, Val loss: 3.916, Epoch time = 89.662s\n",
            "Epoch: 29, Train loss: 2.556, Val loss: 3.911, Epoch time = 89.680s\n",
            "Epoch: 30, Train loss: 2.500, Val loss: 3.902, Epoch time = 89.667s\n",
            "Epoch: 31, Train loss: 2.445, Val loss: 3.906, Epoch time = 89.684s\n",
            "Epoch: 32, Train loss: 2.397, Val loss: 3.889, Epoch time = 89.685s\n",
            "Epoch: 33, Train loss: 2.347, Val loss: 3.878, Epoch time = 89.685s\n",
            "Epoch: 34, Train loss: 2.299, Val loss: 3.868, Epoch time = 89.697s\n",
            "Epoch: 35, Train loss: 2.252, Val loss: 3.859, Epoch time = 89.701s\n",
            "Epoch: 36, Train loss: 2.208, Val loss: 3.847, Epoch time = 89.705s\n",
            "Epoch: 37, Train loss: 2.164, Val loss: 3.852, Epoch time = 89.693s\n",
            "Epoch: 38, Train loss: 2.122, Val loss: 3.859, Epoch time = 92.351s\n",
            "Epoch: 39, Train loss: 2.082, Val loss: 3.862, Epoch time = 90.041s\n",
            "Epoch: 40, Train loss: 2.045, Val loss: 3.873, Epoch time = 90.498s\n",
            "Epoch: 41, Train loss: 2.008, Val loss: 3.863, Epoch time = 90.886s\n",
            "Epoch: 42, Train loss: 1.970, Val loss: 3.878, Epoch time = 91.487s\n",
            "Epoch: 43, Train loss: 1.936, Val loss: 3.871, Epoch time = 91.211s\n",
            "Epoch: 44, Train loss: 1.901, Val loss: 3.885, Epoch time = 92.146s\n",
            "Epoch: 45, Train loss: 1.870, Val loss: 3.897, Epoch time = 91.577s\n",
            "Epoch: 46, Train loss: 1.840, Val loss: 3.896, Epoch time = 90.915s\n",
            "Epoch: 47, Train loss: 1.811, Val loss: 3.898, Epoch time = 90.507s\n",
            "Epoch: 48, Train loss: 1.779, Val loss: 3.920, Epoch time = 91.142s\n",
            "Epoch: 49, Train loss: 1.752, Val loss: 3.935, Epoch time = 90.629s\n",
            "Epoch: 50, Train loss: 1.721, Val loss: 3.946, Epoch time = 90.537s\n",
            "Epoch: 51, Train loss: 1.695, Val loss: 3.980, Epoch time = 91.634s\n",
            "Epoch: 52, Train loss: 1.665, Val loss: 3.989, Epoch time = 91.413s\n",
            "Epoch: 53, Train loss: 1.637, Val loss: 4.030, Epoch time = 91.811s\n",
            "Epoch: 54, Train loss: 1.612, Val loss: 4.054, Epoch time = 91.937s\n",
            "Epoch: 55, Train loss: 1.585, Val loss: 4.085, Epoch time = 91.347s\n",
            "Epoch: 56, Train loss: 1.560, Val loss: 4.110, Epoch time = 91.689s\n",
            "Epoch: 57, Train loss: 1.535, Val loss: 4.136, Epoch time = 91.370s\n",
            "Epoch: 58, Train loss: 1.513, Val loss: 4.150, Epoch time = 90.913s\n",
            "Epoch: 59, Train loss: 1.492, Val loss: 4.179, Epoch time = 91.027s\n",
            "Epoch: 60, Train loss: 1.471, Val loss: 4.161, Epoch time = 90.863s\n",
            "Epoch: 61, Train loss: 1.446, Val loss: 4.196, Epoch time = 91.726s\n",
            "Epoch: 62, Train loss: 1.425, Val loss: 4.212, Epoch time = 91.533s\n",
            "Epoch: 63, Train loss: 1.407, Val loss: 4.251, Epoch time = 91.390s\n",
            "Epoch: 64, Train loss: 1.385, Val loss: 4.273, Epoch time = 91.380s\n",
            "Epoch: 65, Train loss: 1.367, Val loss: 4.240, Epoch time = 91.996s\n",
            "Epoch: 66, Train loss: 1.349, Val loss: 4.250, Epoch time = 92.168s\n",
            "Epoch: 67, Train loss: 1.334, Val loss: 4.251, Epoch time = 91.339s\n",
            "Epoch: 68, Train loss: 1.315, Val loss: 4.242, Epoch time = 91.907s\n",
            "Epoch: 69, Train loss: 1.298, Val loss: 4.266, Epoch time = 91.263s\n",
            "Epoch: 70, Train loss: 1.285, Val loss: 4.263, Epoch time = 90.921s\n",
            "Epoch: 71, Train loss: 1.266, Val loss: 4.271, Epoch time = 90.539s\n",
            "Epoch: 72, Train loss: 1.255, Val loss: 4.273, Epoch time = 90.893s\n",
            "Epoch: 73, Train loss: 1.241, Val loss: 4.282, Epoch time = 90.806s\n",
            "Epoch: 74, Train loss: 1.229, Val loss: 4.292, Epoch time = 90.283s\n",
            "Epoch: 75, Train loss: 1.217, Val loss: 4.290, Epoch time = 90.146s\n",
            "Epoch: 76, Train loss: 1.209, Val loss: 4.310, Epoch time = 90.306s\n",
            "Epoch: 77, Train loss: 1.197, Val loss: 4.315, Epoch time = 91.328s\n",
            "Epoch: 78, Train loss: 1.187, Val loss: 4.324, Epoch time = 91.814s\n",
            "Epoch: 79, Train loss: 1.174, Val loss: 4.314, Epoch time = 90.521s\n",
            "Epoch: 80, Train loss: 1.164, Val loss: 4.311, Epoch time = 91.730s\n",
            "Epoch: 81, Train loss: 1.151, Val loss: 4.310, Epoch time = 89.929s\n",
            "Epoch: 82, Train loss: 1.140, Val loss: 4.294, Epoch time = 90.031s\n",
            "Epoch: 83, Train loss: 1.128, Val loss: 4.287, Epoch time = 91.567s\n",
            "Epoch: 84, Train loss: 1.118, Val loss: 4.299, Epoch time = 90.355s\n",
            "Epoch: 85, Train loss: 1.108, Val loss: 4.294, Epoch time = 92.209s\n",
            "Epoch: 86, Train loss: 1.094, Val loss: 4.278, Epoch time = 92.480s\n",
            "Epoch: 87, Train loss: 1.084, Val loss: 4.278, Epoch time = 90.945s\n",
            "Epoch: 88, Train loss: 1.077, Val loss: 4.311, Epoch time = 91.221s\n",
            "Epoch: 89, Train loss: 1.065, Val loss: 4.289, Epoch time = 90.758s\n",
            "Epoch: 90, Train loss: 1.055, Val loss: 4.284, Epoch time = 89.967s\n"
          ]
        }
      ],
      "source": [
        "epochs = 90\n",
        "losses = []\n",
        "\n",
        "history = {'loss': [], 'val_loss': [], 'time': []}\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(model, optimizer, loss_fn, train_loader)\n",
        "\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model, loss_fn, val_loader)\n",
        "\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "    history['loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['time'].append(epoch_time)\n",
        "\n",
        "    print((f'Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Epoch time = {epoch_time:.3f}s'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.title('Model loss history')\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(device)\n",
        "\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "\n",
        "        prob = model.generator(out[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        \n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate(model: nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "\n",
        "    src_sentence = src_sentence.lower()\n",
        "\n",
        "    src = token_transform(dataset[src_lang].tokenizer, src_sentence)\n",
        "    src = vocab_transform(dataset[src_lang].vocab, src)\n",
        "\n",
        "    src = torch.tensor(src).view(-1, 1)\n",
        "\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "\n",
        "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens+5, start_symbol=SOS_IDX).flatten()\n",
        "\n",
        "    tgt_tokens = [tok for tok in tgt_tokens.cpu().numpy() if tok not in (SOS_IDX, EOS_IDX)]\n",
        "\n",
        "    return ' '.join([list(dataset[tgt_lang].vocab.keys())[tok] for tok in tgt_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input  (en): why are you so late\n",
            "output (id): mengapa anda terlambat\n",
            "\n",
            "input  (en): what do you know\n",
            "output (id): apa yang anda ketahui\n",
            "\n",
            "input  (en): it's a good day\n",
            "output (id): hari ini sudah baik\n",
            "\n",
            "input  (en): The decisive difference was revealed in the midst of the war\n",
            "output (id): dalam estimasi selisih tersebut ditemukan pada pertengahan empat dari perang\n",
            "\n",
            "input  (en): everything written on them was unrepeatable since time immemorial and forever more\n",
            "output (id): segala sesuatu yang tertera sejak awal tahun tentang penyebab gagasan\n",
            "\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "    'why are you so late',\n",
        "    'what do you know',\n",
        "    'it\\'s a good day',\n",
        "    'The decisive difference was revealed in the midst of the war',\n",
        "    'everything written on them was unrepeatable since time immemorial and forever more',\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    print(f'input  (en): {text}')\n",
        "    print(f'output (id): {translate(model, text)}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input  (en): you're so smart\n",
            "output (id): anda terlalu pintar\n",
            "\n",
            "input  (en): you're pretty smart\n",
            "output (id): anda merasa cukup pintar\n",
            "\n",
            "input  (en): this is a beginning\n",
            "output (id): pada awal ini terdapat awal\n",
            "\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "    'you\\'re so smart',\n",
        "    'you\\'re pretty smart',\n",
        "    'this is a beginning',\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    print(f'input  (en): {text}')\n",
        "    print(f'output (id): {translate(model, text)}')\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
