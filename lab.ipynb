{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from munch import Munch\n",
    "from dataset import Dataset\n",
    "\n",
    "# load dataset\n",
    "data_path = Path('data')\n",
    "data_files = {'en': [], 'id': []}\n",
    "\n",
    "for name in data_path.iterdir():\n",
    "    for lang in data_files.keys():\n",
    "        lang_path = name / lang\n",
    "\n",
    "        for dir in lang_path.iterdir():\n",
    "            if dir.is_file():\n",
    "                with open(dir, 'rb') as f:\n",
    "                    content = f.read().decode('UTF-8')\n",
    "\n",
    "                    data_files[lang].append(Munch(\n",
    "                        name=str(dir.name),\n",
    "                        path=str(dir),\n",
    "                        content=content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed documents: 100%|██████████| 20/20 [36:30<00:00, 109.54s/it]  \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def build_dataset(docs):\n",
    "    vocab_idx = Counter(['<bos>', '<eos>'])\n",
    "    seq_words = []\n",
    "\n",
    "    for doc in tqdm(docs, 'processed documents'):\n",
    "        sentences = sent_tokenize(doc)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent)\n",
    "\n",
    "            seq_words.append(words)\n",
    "            vocab_idx.update(words)\n",
    "    \n",
    "    vocab_idx = dict(vocab_idx)\n",
    "    count = len(vocab_idx)\n",
    "\n",
    "    vocab = dict()\n",
    "\n",
    "    for i, k in enumerate(vocab_idx.keys()):\n",
    "        ohe = [0] * count\n",
    "        ohe[i] = 1\n",
    "\n",
    "        vocab_idx[k] = i\n",
    "        vocab[k] = ohe\n",
    "\n",
    "    seq_tensors = []\n",
    "\n",
    "    BOS_IDX = vocab['<bos>']\n",
    "    EOS_IDX = vocab['<eos>']\n",
    "\n",
    "    for words in seq_words:\n",
    "        seq_tensors.append(torch.tensor([vocab[BOS_IDX]] + [vocab[w] for w in words]) + [vocab[EOS_IDX]])\n",
    "        \n",
    "    return Munch(vocab=vocab, data=seq_words, tensors=seq_tensors)\n",
    "\n",
    "data_en = build_dataset([doc.content for doc in data_files['en']])\n",
    "data_id = build_dataset([doc.content for doc in data_files['id']])\n",
    "\n",
    "dataset = (data_en, data_id)\n",
    "\n",
    "with open('dataset', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'typeof(data_filesasdfsafdaf0].content)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# from nltk import sent_tokenize, word_tokenize\n",
    "# import codecs\n",
    "\n",
    "# sentences = sent_tokenize(data_files['en'][0].content)\n",
    "# words = word_tokenize(sentences[0])\n",
    "\n",
    "# print(words)\n",
    "\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# english_tokenizer = get_tokenizer('spacy', language='en')\n",
    "# indonesian_tokenizer = get_tokenizer('spacy', language='in')\n",
    "\n",
    "# def build_vocabulary(filepath, tokenizer):\n",
    "#   counter = Counter()\n",
    "#   with io.open(filepath, encoding=\"utf8\") as f:\n",
    "#     for string_ in f:\n",
    "#       counter.update(tokenizer(string_))\n",
    "#   return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "# english_vocabularies = build_vocabulary(data_files['en'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "gensim_model = Word2Vec(sentences=data_files['en'],\n",
    "                        vector_size=100,\n",
    "                        window=5,\n",
    "                        min_count=1,\n",
    "                        workers=4)\n",
    "\n",
    "print(gensim_model[\"awesome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid,\n",
    "                 nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0027c5f14629e3b6ac48217581d6af8c5141a381107928960a1f235c4a8489da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
