{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload modules on every saved changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parallel-corpora-en-id-master.zip: 18.4MiB [00:01, 10.8MiB/s]\n",
      "Indonesian-English-Bilingual-Corpus-master.zip: 3.00MiB [00:00, 6.21MiB/s]\n",
      "TALPCo-master.zip: 717kiB [00:01, 553kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "from core.dataset import Dataset\n",
    "\n",
    "Dataset.download_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data (en): 100%|██████████| 315508/315508 [00:32<00:00, 9729.01it/s] \n",
      "Tokenizing data (id): 100%|██████████| 315508/315508 [00:39<00:00, 7997.76it/s] \n"
     ]
    }
   ],
   "source": [
    "from core.dataset import Dataset\n",
    "\n",
    "SRC_LANG = 'en'\n",
    "TGT_LANG = 'id'\n",
    "\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "dataset = Dataset.load_dataset(init_vocab=['<unk>', '<pad>', '<sos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the opposite of \"heavy\" is \"light\".</td>\n",
       "      <td>lawan kata \"berat\" adalah \"ringan\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you cannot go into this room.</td>\n",
       "      <td>tidak boleh masuk kamar ini.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you press here, the sound will get louder.</td>\n",
       "      <td>kalau ditekan di sini, bunyinya akan menjadi b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mr. lee and i are in the same group.</td>\n",
       "      <td>saya sekelompok dengan bapak lee.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i always read a book for only thirty minutes b...</td>\n",
       "      <td>saya selalu membaca buku tiga puluh menit saja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i ate only one snack.</td>\n",
       "      <td>saya makan kue sebuah saja.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b: one, two, three, four...</td>\n",
       "      <td>b: satu, dua, tiga, empat....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a: how many are there?</td>\n",
       "      <td>a: ada berapa?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>there are seven eggs in the refrigerator.</td>\n",
       "      <td>di dalam kulkas ada tujuh butir telur.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i always go to work at nine in the morning.</td>\n",
       "      <td>saya pergi ke perusahaan setiap pagi pukul sem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                the opposite of \"heavy\" is \"light\".   \n",
       "1                      you cannot go into this room.   \n",
       "2      if you press here, the sound will get louder.   \n",
       "3               mr. lee and i are in the same group.   \n",
       "4  i always read a book for only thirty minutes b...   \n",
       "5                              i ate only one snack.   \n",
       "6                        b: one, two, three, four...   \n",
       "7                             a: how many are there?   \n",
       "8          there are seven eggs in the refrigerator.   \n",
       "9        i always go to work at nine in the morning.   \n",
       "\n",
       "                                                  id  \n",
       "0                lawan kata \"berat\" adalah \"ringan\".  \n",
       "1                       tidak boleh masuk kamar ini.  \n",
       "2  kalau ditekan di sini, bunyinya akan menjadi b...  \n",
       "3                  saya sekelompok dengan bapak lee.  \n",
       "4  saya selalu membaca buku tiga puluh menit saja...  \n",
       "5                        saya makan kue sebuah saja.  \n",
       "6                      b: satu, dua, tiga, empat....  \n",
       "7                                     a: ada berapa?  \n",
       "8             di dalam kulkas ada tujuh butir telur.  \n",
       "9  saya pergi ke perusahaan setiap pagi pukul sem...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.DataFrame(np.transpose([dataset.data[lang].sent[-10:] for lang in dataset.langs]), columns=dataset.langs).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab (en): 6667439\n",
      "vocab (id): 5841565\n"
     ]
    }
   ],
   "source": [
    "for lang in dataset.langs:\n",
    "    print('vocab ({}): {}'.format(lang, len(dataset.data[lang].vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import math\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, max_length: int=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pos = torch.arange(0, max_length).reshape(max_length, 1)\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "\n",
    "        pos_emb = torch.zeros((max_length, emb_size))\n",
    "        pos_emb[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_emb[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_emb = pos_emb.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_emb', pos_emb)\n",
    "\n",
    "    def forward(self, token_emb):\n",
    "        return self.dropout(token_emb + self.pos_emb[:token_emb.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.emb(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                encoder_layers: int,\n",
    "                decode_layers: int,\n",
    "                emb_size: int,\n",
    "                nhead: int,\n",
    "                source_vocab_size: int,\n",
    "                target_vocab_size: int,\n",
    "                dim_feedforward: int,\n",
    "                dropout: float):\n",
    "                \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "        self.source_token_emb = TokenEmbedding(source_vocab_size, emb_size)\n",
    "        self.target_token_emb = TokenEmbedding(target_vocab_size, emb_size)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model=emb_size,\n",
    "                                    nhead=nhead,\n",
    "                                    num_encoder_layers=encoder_layers,\n",
    "                                    num_decoder_layers=decode_layers,\n",
    "                                    dim_feedforward=dim_feedforward,\n",
    "                                    dropout=dropout)\n",
    "\n",
    "        self.generator = nn.Linear(emb_size, target_vocab_size)\n",
    "    \n",
    "    def forward(self,\n",
    "                source: Tensor,\n",
    "                target: Tensor,\n",
    "                source_mask: Tensor,\n",
    "                target_mask: Tensor,\n",
    "                source_padding_mask: Tensor,\n",
    "                target_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "\n",
    "        source_emb = self.pos_encoding(self.source_token_emb(source))\n",
    "        target_emb = self.pos_encoding(self.target_token_emb(target))\n",
    "\n",
    "        out_seq = self.transformer(source_emb, target_emb, source_mask, target_mask,\n",
    "                                None, source_padding_mask, target_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "        return self.generator(out_seq)\n",
    "\n",
    "    def encode(self, source: Tensor, source_mask: Tensor):\n",
    "        return self.transformer.encoder(self.pos_encoding(self.source_token_emb(source)), source_mask)\n",
    "    \n",
    "    def decode(self, target: Tensor, memory: Tensor, target_mask: Tensor):\n",
    "        return self.transformer.decoder(self.pos_encoding(self.target_token_emb(target)), memory, target_mask)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(source, target):\n",
    "    source_seq_len = source.shape[0]\n",
    "    target_seq_len = target.shape[0]\n",
    "\n",
    "    target_mask = generate_square_subsequent_mask(target_seq_len)\n",
    "    source_mask = torch.zeros((source_seq_len, source_seq_len),device=device).type(torch.bool)\n",
    "\n",
    "    source_padding_mask = (source == PAD_IDX).transpose(0, 1)\n",
    "    target_padding_mask = (target == PAD_IDX).transpose(0, 1)\n",
    "    return source_mask, target_mask, source_padding_mask, target_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(3, 3, 512, 8,\n",
    "                            len(dataset.data[SRC_LANG].vocab), len(dataset.data[TGT_LANG].vocab), 512, 0.1)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1236/1973926823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTGT_LANG\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTGT_LANG\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtensor_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Size mismatch between tensors\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def vocab_tf(vocab, tokens):\n",
    "    return [SOS_IDX] + [vocab[tok] if tok in vocab.keys() else UNK_IDX for tok in tokens] + [EOS_IDX]\n",
    "\n",
    "def feature_tensors(vocab, sent_tokens):\n",
    "    tensors = [torch.Tensor(vocab_tf(vocab, tokens)) for tokens in sent_tokens]\n",
    "\n",
    "    return pad_sequence(tensors, padding_value=PAD_IDX)\n",
    "\n",
    "source = feature_tensors(dataset.data[SRC_LANG].vocab, dataset.data[SRC_LANG].sent_tokens)\n",
    "target = feature_tensors(dataset.data[TGT_LANG].vocab, dataset.data[TGT_LANG].sent_tokens)\n",
    "\n",
    "tensor_dataset = TensorDataset(source, target)\n",
    "dataloader = DataLoader(tensor_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\"))\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and Accuracy\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0027c5f14629e3b6ac48217581d6af8c5141a381107928960a1f235c4a8489da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
