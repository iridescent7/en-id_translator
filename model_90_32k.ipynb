{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kpYwy7woFX8l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Indonesian-English-Bilingual-Corpus-master.zip: 100%|██████████| 3.00M/3.00M [00:01<00:00, 1.75MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished downloading\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import os\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from munch import Munch\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def download_file(url: str, path: Path):\n",
        "    resp = requests.get(url, stream=True)\n",
        "\n",
        "    content_disp = resp.headers['content-disposition']\n",
        "\n",
        "    if content_disp:\n",
        "        pattern = 'filename='\n",
        "        index = content_disp.find(pattern)\n",
        "\n",
        "        if index > -1:\n",
        "            name = content_disp[index+len(pattern):]\n",
        "        else:\n",
        "            name = url.split('/')[-1]\n",
        "    else:\n",
        "        name = url.split('/')[-1]\n",
        "\n",
        "    dest = path / name\n",
        "\n",
        "    tqdm_args = {\n",
        "        'desc': name,\n",
        "        'total': int(resp.headers.get('content-length', 0)),\n",
        "        'unit': 'iB',\n",
        "        'unit_scale': True,\n",
        "        'unit_divisor': 1024\n",
        "    }\n",
        "\n",
        "    with open(dest, 'wb') as file, tqdm(**tqdm_args) as bar:\n",
        "        for data in resp.iter_content(chunk_size=1024):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "    return dest\n",
        "\n",
        "def _parallel_dataset(path: Path, prefix: str='parallel'):\n",
        "    url = 'https://codeload.github.com/prasastoadi/parallel-corpora-en-id/zip/refs/heads/master'\n",
        "    dest = download_file(url, path)\n",
        "\n",
        "    with zipfile.ZipFile(dest, 'r') as zf:\n",
        "        for f in zf.namelist():\n",
        "            if f.endswith('.tgz'):\n",
        "\n",
        "                with zf.open(f, 'r') as fo, tarfile.open(fileobj=fo, mode='r') as tf:\n",
        "                    for m in tf.getmembers():\n",
        "                        if m.name.endswith('.en') or '-EN-' in m.name:\n",
        "                            tf.extract(m, path=path / prefix /'en')\n",
        "                        else:\n",
        "                            tf.extract(m, path=path / prefix / 'id')\n",
        "\n",
        "    dest.unlink()\n",
        "    \n",
        "def _bilingual_dataset(path: Path, prefix: str='bilingual'):\n",
        "    url = 'https://github.com/desmond86/Indonesian-English-Bilingual-Corpus/archive/refs/heads/master.zip'\n",
        "    dest = download_file(url, path)\n",
        "\n",
        "    with zipfile.ZipFile(dest, 'r') as zf:\n",
        "        for f in zf.namelist():\n",
        "            en = f.endswith('.en')\n",
        "            id = f.endswith('.id')\n",
        "\n",
        "            if en or id:\n",
        "                dest_path = path / prefix / ('en' if en else 'id')\n",
        "                dest_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with zf.open(f, 'r') as fo, open(dest_path / os.path.basename(f), 'wb') as t:\n",
        "                    t.write(fo.read())\n",
        "\n",
        "    dest.unlink()\n",
        "    \n",
        "def _talpco_dataset(path: Path, prefix: str='talpco'):\n",
        "    url = 'https://github.com/matbahasa/TALPCo/archive/refs/heads/master.zip'\n",
        "    dest = download_file(url, path)\n",
        "\n",
        "    path_en = path / prefix / 'en'\n",
        "    path_id = path / prefix / 'id'\n",
        "\n",
        "    path_en.mkdir(parents=True, exist_ok=True)\n",
        "    path_id.mkdir(exist_ok=True)\n",
        "\n",
        "    file_to_path = [\n",
        "        ('data_eng.txt', path_en),\n",
        "        ('data_ind.txt', path_id)\n",
        "    ]\n",
        "\n",
        "    with zipfile.ZipFile(dest, 'r') as zf:\n",
        "        for f in zf.namelist():\n",
        "            for fn, p in file_to_path:\n",
        "                if f.endswith(fn):\n",
        "                    \n",
        "                    with zf.open(f, 'r') as fo, open(p / fn, 'w') as t:\n",
        "                        items = []\n",
        "                        for line in fo.readlines():\n",
        "                            split = line.decode('UTF-8').split('\\t')\n",
        "            \n",
        "                            if len(split) > 1:\n",
        "                                items.append(split[1].replace('\\r', ''))\n",
        "                        \n",
        "                        t.writelines(items)\n",
        "                        \n",
        "    dest.unlink()\n",
        "    \n",
        "def download_all(path: str='raw_data'):\n",
        "    path = Path(path)\n",
        "    path.mkdir(exist_ok=True)\n",
        "    \n",
        "    print('Downloading data')\n",
        "\n",
        "    # _parallel_dataset(path)\n",
        "    _bilingual_dataset(path)\n",
        "    # _talpco_dataset(path)\n",
        "\n",
        "    print('Finished downloading')\n",
        "\n",
        "download_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building dataset\n",
            "Finished building dataset\n"
          ]
        }
      ],
      "source": [
        "def build_dataset(path='data',\n",
        "                    data_path='raw_data',\n",
        "                    langs=['en', 'id'],\n",
        "                    test_size=0.2,\n",
        "                    random_state=42):\n",
        "    path = Path(path)        \n",
        "    data_path = Path(data_path)\n",
        "\n",
        "    if not data_path.exists():\n",
        "        raise RuntimeError('No data is found, please download dataset first')\n",
        "\n",
        "    path.mkdir(exist_ok=True)\n",
        "\n",
        "    data = dict()\n",
        "    last_len = -1\n",
        "\n",
        "    print('Building dataset')\n",
        "\n",
        "    for lang in langs:\n",
        "        sent = []\n",
        "        \n",
        "        for name in data_path.iterdir():\n",
        "            for d in (name / lang).iterdir():\n",
        "                if d.is_file():\n",
        "                    with open(d, 'r', encoding='UTF-8') as file:\n",
        "                        # All sentences are already separated by a newline\n",
        "                        for line in file.readlines():\n",
        "                            sent.append(line.lower())\n",
        "\n",
        "        data[lang] = np.array(sent)\n",
        "\n",
        "        if last_len == -1:\n",
        "            last_len = len(sent)\n",
        "        elif last_len != len(sent):\n",
        "            raise RuntimeError('Data length of sentence mismatch ({} -> {} ({}))'.format(last_len, len(sent), lang))\n",
        "\n",
        "\n",
        "    state = np.random.get_state()\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    msk = np.random.rand(last_len) > test_size\n",
        "    np.random.set_state(state)\n",
        "    \n",
        "    for lang in langs:\n",
        "        with open(path / (lang + '_train.txt'), 'w', encoding='UTF-8') as file:\n",
        "            file.writelines(data[lang][msk])\n",
        "            \n",
        "        with open(path / (lang + '_test.txt'), 'w', encoding='UTF-8') as file:\n",
        "            file.writelines(data[lang][~msk])\n",
        "\n",
        "    print('Finished building dataset')\n",
        "\n",
        "build_dataset(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import math\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, max_length: int=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pos = torch.arange(0, max_length).reshape(max_length, 1)\n",
        "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "\n",
        "        pos_emb = torch.zeros((max_length, emb_size))\n",
        "        pos_emb[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_emb[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_emb = pos_emb.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_emb', pos_emb)\n",
        "\n",
        "    def forward(self, token_emb):\n",
        "        return self.dropout(token_emb + self.pos_emb[:token_emb.size(0), :])\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.emb(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                encoder_layers: int,\n",
        "                decode_layers: int,\n",
        "                emb_size: int,\n",
        "                nhead: int,\n",
        "                src_vocab_size: int,\n",
        "                tgt_vocab_size: int,\n",
        "                dim_feedforward: int,\n",
        "                dropout: float):\n",
        "                \n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "        self.src_token_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_token_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        \n",
        "        self.transformer = nn.Transformer(d_model=emb_size,\n",
        "                                          nhead=nhead,\n",
        "                                          num_encoder_layers=encoder_layers,\n",
        "                                          num_decoder_layers=decode_layers,\n",
        "                                          dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout)\n",
        "\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "    \n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                tgt: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_pad_mask: Tensor,\n",
        "                tgt_pad_mask: Tensor,\n",
        "                mem_key_pad_mask: Tensor):\n",
        "\n",
        "        src_emb = self.pos_encoding(self.src_token_emb(src))\n",
        "        tgt_emb = self.pos_encoding(self.tgt_token_emb(tgt))\n",
        "\n",
        "        out_seq = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                   src_pad_mask, tgt_pad_mask,\n",
        "                                   mem_key_pad_mask)\n",
        "\n",
        "        return self.generator(out_seq)\n",
        "        \n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.pos_encoding(self.src_token_emb(src)), src_mask)\n",
        "    \n",
        "    def decode(self, tgt: Tensor, mem: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.pos_encoding(self.tgt_token_emb(tgt)), mem, tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(size):\n",
        "    mask = (torch.triu(torch.ones((size, size), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, -1e9).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
        "\n",
        "    src_pad_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_pad_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "\n",
        "    return src_mask, tgt_mask, src_pad_mask, tgt_pad_mask\n",
        "\n",
        "def train_epoch(model, opt, loss_fn, train_loader):\n",
        "    model.train()\n",
        "    losses = 0.0\n",
        "\n",
        "    for src, tgt in train_loader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        opt.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_loader)\n",
        "\n",
        "def evaluate(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    losses = 0.0\n",
        "\n",
        "    for src, tgt in val_loader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing data (en/train): 100%|██████████| 32339/32339 [00:04<00:00, 6897.58it/s]\n",
            "Tokenizing data (en/test): 100%|██████████| 8030/8030 [00:00<00:00, 8245.27it/s]\n",
            "Tokenizing data (id/train): 100%|██████████| 32339/32339 [00:05<00:00, 5998.71it/s]\n",
            "Tokenizing data (id/test): 100%|██████████| 8030/8030 [00:01<00:00, 7766.45it/s]\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "specials = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
        "\n",
        "def token_transform(tokenizer, sent):\n",
        "    tokens = []\n",
        "    for tok in tokenizer(sent):\n",
        "        tok = str(tok)\n",
        "\n",
        "        if any(c.isdigit() for c in tok):\n",
        "            continue\n",
        "\n",
        "        if all(c in string.punctuation for c in tok):\n",
        "            continue\n",
        "\n",
        "        tokens.append(tok)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def vocab_transform(vocab, sent_tok):\n",
        "    return [SOS_IDX] + \\\n",
        "           [vocab[tok] if tok in vocab.keys() else UNK_IDX for tok in sent_tok] + \\\n",
        "           [EOS_IDX]\n",
        "\n",
        "def load_dataset(path: str='data',\n",
        "                 src_lang: str='en',\n",
        "                 tgt_lang: str='id'):\n",
        "    path = Path(path)\n",
        "\n",
        "    if not path.exists():\n",
        "        raise RuntimeError('No data is found, please download and/or build dataset first')\n",
        "\n",
        "    data = dict()\n",
        "\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        token_ids = dict()\n",
        "        vocab = dict()\n",
        "\n",
        "        try:\n",
        "            tokenizer = spacy.load(lang)\n",
        "        except:\n",
        "            tokenizer = spacy.blank(lang)\n",
        "\n",
        "        for type in ['train', 'test']:\n",
        "            sentences = []\n",
        "\n",
        "            with open(path / (lang + '_' + type + '.txt'), 'r', encoding='UTF-8') as file:\n",
        "                for line in file.readlines():\n",
        "                    sentences.append(line.lower().rstrip('\\n'))\n",
        "\n",
        "            sent_tokens = []\n",
        "\n",
        "            for sent in tqdm(sentences, f'Tokenizing data ({lang}/{type})'):\n",
        "                sent_tokens.append(token_transform(tokenizer, sent))\n",
        "\n",
        "            if type == 'train':\n",
        "                counter = Counter()\n",
        "                for tokens in sent_tokens:\n",
        "                    counter.update(tokens)\n",
        "\n",
        "                for tok in specials:\n",
        "                    del counter[tok]\n",
        "\n",
        "                sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[0])\n",
        "                sorted_by_freq_tuples.sort(key=lambda x: x[1], reverse=True)\n",
        "                ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "                for symbol in specials[::-1]:\n",
        "                    ordered_dict.update({symbol: 1})\n",
        "                    ordered_dict.move_to_end(symbol, last=False)\n",
        "\n",
        "                vocab = dict(zip(ordered_dict, range(len(ordered_dict))))\n",
        "\n",
        "            token_ids[type] = []\n",
        "            for sent_tok in sent_tokens:\n",
        "                token_ids[type].append(vocab_transform(vocab, sent_tok))\n",
        "\n",
        "        data[lang] = Munch(vocab=vocab, data=token_ids, tokenizer=tokenizer)\n",
        "\n",
        "    return data\n",
        "\n",
        "dataset = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "for lang in ['en', 'id']:\n",
        "    with open(lang + '_vocab.txt', 'w', encoding='UTF-8') as f:\n",
        "        f.writelines('\\n'.join(dataset[lang].vocab.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn8tWAAPGWBA",
        "outputId": "79a5d646-a02e-4084-8821-ba321ea34d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src vocab size: 18727, tgt vocab size: 18586\n"
          ]
        }
      ],
      "source": [
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "src_lang = 'en'\n",
        "tgt_lang = 'id'\n",
        "\n",
        "train_batch_size = 40\n",
        "val_batch_size = 40\n",
        "\n",
        "class SequenceDataset(data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def get_pair_set(data, type):\n",
        "    return [(x, y) for x, y in zip(data[src_lang].data[type], data[tgt_lang].data[type])]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(torch.tensor(src_sample))\n",
        "        tgt_batch.append(torch.tensor(tgt_sample))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    \n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "train_loader = data.DataLoader(dataset=SequenceDataset(get_pair_set(dataset, 'train')),\n",
        "                        batch_size=train_batch_size,\n",
        "                        collate_fn=collate_fn)\n",
        "\n",
        "val_loader = data.DataLoader(dataset=SequenceDataset(get_pair_set(dataset, 'test')),\n",
        "                        batch_size=val_batch_size,\n",
        "                        collate_fn=collate_fn)\n",
        "\n",
        "src_vocab_size = len(dataset[src_lang].vocab)\n",
        "tgt_vocab_size = len(dataset[tgt_lang].vocab)\n",
        "\n",
        "print('src vocab size: {}, tgt vocab size: {}'.format(src_vocab_size, tgt_vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9n5uLRafH41s"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Seq2SeqTransformer(3, 3, 256, 8,\n",
        "                           src_vocab_size, tgt_vocab_size,\n",
        "                           256, 0.1)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 7.326, Val loss: 7.071, Epoch time = 81.908s\n",
            "Epoch: 2, Train loss: 6.650, Val loss: 6.819, Epoch time = 75.558s\n",
            "Epoch: 3, Train loss: 6.292, Val loss: 6.580, Epoch time = 75.631s\n",
            "Epoch: 4, Train loss: 5.973, Val loss: 6.263, Epoch time = 76.119s\n",
            "Epoch: 5, Train loss: 5.682, Val loss: 6.005, Epoch time = 75.606s\n",
            "Epoch: 6, Train loss: 5.409, Val loss: 5.747, Epoch time = 75.652s\n",
            "Epoch: 7, Train loss: 5.157, Val loss: 5.491, Epoch time = 75.624s\n",
            "Epoch: 8, Train loss: 4.923, Val loss: 5.282, Epoch time = 75.688s\n",
            "Epoch: 9, Train loss: 4.712, Val loss: 5.101, Epoch time = 75.306s\n",
            "Epoch: 10, Train loss: 4.517, Val loss: 4.936, Epoch time = 89.099s\n",
            "Epoch: 11, Train loss: 4.338, Val loss: 4.788, Epoch time = 89.914s\n",
            "Epoch: 12, Train loss: 4.167, Val loss: 4.671, Epoch time = 89.906s\n",
            "Epoch: 13, Train loss: 4.013, Val loss: 4.555, Epoch time = 89.903s\n",
            "Epoch: 14, Train loss: 3.868, Val loss: 4.466, Epoch time = 89.909s\n",
            "Epoch: 15, Train loss: 3.733, Val loss: 4.390, Epoch time = 89.907s\n",
            "Epoch: 16, Train loss: 3.612, Val loss: 4.293, Epoch time = 89.974s\n",
            "Epoch: 17, Train loss: 3.495, Val loss: 4.240, Epoch time = 89.690s\n",
            "Epoch: 18, Train loss: 3.394, Val loss: 4.170, Epoch time = 89.686s\n",
            "Epoch: 19, Train loss: 3.291, Val loss: 4.123, Epoch time = 89.697s\n",
            "Epoch: 20, Train loss: 3.197, Val loss: 4.072, Epoch time = 89.681s\n",
            "Epoch: 21, Train loss: 3.106, Val loss: 4.055, Epoch time = 89.702s\n",
            "Epoch: 22, Train loss: 3.021, Val loss: 4.025, Epoch time = 89.688s\n",
            "Epoch: 23, Train loss: 2.942, Val loss: 4.003, Epoch time = 89.688s\n",
            "Epoch: 24, Train loss: 2.869, Val loss: 3.979, Epoch time = 89.681s\n",
            "Epoch: 25, Train loss: 2.798, Val loss: 3.968, Epoch time = 89.685s\n",
            "Epoch: 26, Train loss: 2.734, Val loss: 3.932, Epoch time = 89.713s\n",
            "Epoch: 27, Train loss: 2.671, Val loss: 3.934, Epoch time = 89.686s\n",
            "Epoch: 28, Train loss: 2.610, Val loss: 3.916, Epoch time = 89.662s\n",
            "Epoch: 29, Train loss: 2.556, Val loss: 3.911, Epoch time = 89.680s\n",
            "Epoch: 30, Train loss: 2.500, Val loss: 3.902, Epoch time = 89.667s\n",
            "Epoch: 31, Train loss: 2.445, Val loss: 3.906, Epoch time = 89.684s\n",
            "Epoch: 32, Train loss: 2.397, Val loss: 3.889, Epoch time = 89.685s\n",
            "Epoch: 33, Train loss: 2.347, Val loss: 3.878, Epoch time = 89.685s\n",
            "Epoch: 34, Train loss: 2.299, Val loss: 3.868, Epoch time = 89.697s\n",
            "Epoch: 35, Train loss: 2.252, Val loss: 3.859, Epoch time = 89.701s\n",
            "Epoch: 36, Train loss: 2.208, Val loss: 3.847, Epoch time = 89.705s\n",
            "Epoch: 37, Train loss: 2.164, Val loss: 3.852, Epoch time = 89.693s\n",
            "Epoch: 38, Train loss: 2.122, Val loss: 3.859, Epoch time = 92.351s\n",
            "Epoch: 39, Train loss: 2.082, Val loss: 3.862, Epoch time = 90.041s\n",
            "Epoch: 40, Train loss: 2.045, Val loss: 3.873, Epoch time = 90.498s\n",
            "Epoch: 41, Train loss: 2.008, Val loss: 3.863, Epoch time = 90.886s\n",
            "Epoch: 42, Train loss: 1.970, Val loss: 3.878, Epoch time = 91.487s\n",
            "Epoch: 43, Train loss: 1.936, Val loss: 3.871, Epoch time = 91.211s\n",
            "Epoch: 44, Train loss: 1.901, Val loss: 3.885, Epoch time = 92.146s\n",
            "Epoch: 45, Train loss: 1.870, Val loss: 3.897, Epoch time = 91.577s\n",
            "Epoch: 46, Train loss: 1.840, Val loss: 3.896, Epoch time = 90.915s\n",
            "Epoch: 47, Train loss: 1.811, Val loss: 3.898, Epoch time = 90.507s\n",
            "Epoch: 48, Train loss: 1.779, Val loss: 3.920, Epoch time = 91.142s\n",
            "Epoch: 49, Train loss: 1.752, Val loss: 3.935, Epoch time = 90.629s\n",
            "Epoch: 50, Train loss: 1.721, Val loss: 3.946, Epoch time = 90.537s\n",
            "Epoch: 51, Train loss: 1.695, Val loss: 3.980, Epoch time = 91.634s\n",
            "Epoch: 52, Train loss: 1.665, Val loss: 3.989, Epoch time = 91.413s\n",
            "Epoch: 53, Train loss: 1.637, Val loss: 4.030, Epoch time = 91.811s\n",
            "Epoch: 54, Train loss: 1.612, Val loss: 4.054, Epoch time = 91.937s\n",
            "Epoch: 55, Train loss: 1.585, Val loss: 4.085, Epoch time = 91.347s\n",
            "Epoch: 56, Train loss: 1.560, Val loss: 4.110, Epoch time = 91.689s\n",
            "Epoch: 57, Train loss: 1.535, Val loss: 4.136, Epoch time = 91.370s\n",
            "Epoch: 58, Train loss: 1.513, Val loss: 4.150, Epoch time = 90.913s\n",
            "Epoch: 59, Train loss: 1.492, Val loss: 4.179, Epoch time = 91.027s\n",
            "Epoch: 60, Train loss: 1.471, Val loss: 4.161, Epoch time = 90.863s\n",
            "Epoch: 61, Train loss: 1.446, Val loss: 4.196, Epoch time = 91.726s\n",
            "Epoch: 62, Train loss: 1.425, Val loss: 4.212, Epoch time = 91.533s\n",
            "Epoch: 63, Train loss: 1.407, Val loss: 4.251, Epoch time = 91.390s\n",
            "Epoch: 64, Train loss: 1.385, Val loss: 4.273, Epoch time = 91.380s\n",
            "Epoch: 65, Train loss: 1.367, Val loss: 4.240, Epoch time = 91.996s\n",
            "Epoch: 66, Train loss: 1.349, Val loss: 4.250, Epoch time = 92.168s\n",
            "Epoch: 67, Train loss: 1.334, Val loss: 4.251, Epoch time = 91.339s\n",
            "Epoch: 68, Train loss: 1.315, Val loss: 4.242, Epoch time = 91.907s\n",
            "Epoch: 69, Train loss: 1.298, Val loss: 4.266, Epoch time = 91.263s\n",
            "Epoch: 70, Train loss: 1.285, Val loss: 4.263, Epoch time = 90.921s\n",
            "Epoch: 71, Train loss: 1.266, Val loss: 4.271, Epoch time = 90.539s\n",
            "Epoch: 72, Train loss: 1.255, Val loss: 4.273, Epoch time = 90.893s\n",
            "Epoch: 73, Train loss: 1.241, Val loss: 4.282, Epoch time = 90.806s\n",
            "Epoch: 74, Train loss: 1.229, Val loss: 4.292, Epoch time = 90.283s\n",
            "Epoch: 75, Train loss: 1.217, Val loss: 4.290, Epoch time = 90.146s\n",
            "Epoch: 76, Train loss: 1.209, Val loss: 4.310, Epoch time = 90.306s\n",
            "Epoch: 77, Train loss: 1.197, Val loss: 4.315, Epoch time = 91.328s\n",
            "Epoch: 78, Train loss: 1.187, Val loss: 4.324, Epoch time = 91.814s\n",
            "Epoch: 79, Train loss: 1.174, Val loss: 4.314, Epoch time = 90.521s\n",
            "Epoch: 80, Train loss: 1.164, Val loss: 4.311, Epoch time = 91.730s\n",
            "Epoch: 81, Train loss: 1.151, Val loss: 4.310, Epoch time = 89.929s\n",
            "Epoch: 82, Train loss: 1.140, Val loss: 4.294, Epoch time = 90.031s\n",
            "Epoch: 83, Train loss: 1.128, Val loss: 4.287, Epoch time = 91.567s\n",
            "Epoch: 84, Train loss: 1.118, Val loss: 4.299, Epoch time = 90.355s\n",
            "Epoch: 85, Train loss: 1.108, Val loss: 4.294, Epoch time = 92.209s\n",
            "Epoch: 86, Train loss: 1.094, Val loss: 4.278, Epoch time = 92.480s\n",
            "Epoch: 87, Train loss: 1.084, Val loss: 4.278, Epoch time = 90.945s\n",
            "Epoch: 88, Train loss: 1.077, Val loss: 4.311, Epoch time = 91.221s\n",
            "Epoch: 89, Train loss: 1.065, Val loss: 4.289, Epoch time = 90.758s\n",
            "Epoch: 90, Train loss: 1.055, Val loss: 4.284, Epoch time = 89.967s\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "epochs = 90\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(model, optimizer, loss_fn, train_loader)\n",
        "\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model, loss_fn, val_loader)\n",
        "\n",
        "    print((f'Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model_90 epoch.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load('model_90 epoch.ckpt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'kau masih belum berfungsi'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(device)\n",
        "\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "\n",
        "        prob = model.generator(out[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        \n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate(model: nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "\n",
        "    src = token_transform(dataset[src_lang].tokenizer, src_sentence)\n",
        "    src = vocab_transform(dataset[src_lang].vocab, src)\n",
        "\n",
        "    src = torch.tensor(src).view(-1, 1)\n",
        "\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "\n",
        "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens+5, start_symbol=SOS_IDX).flatten()\n",
        "\n",
        "    tgt_tokens = [tok for tok in tgt_tokens.cpu().numpy() if tok not in (SOS_IDX, EOS_IDX)]\n",
        "\n",
        "    return ' '.join([list(dataset[tgt_lang].vocab.keys())[tok] for tok in tgt_tokens])\n",
        "\n",
        "translate(model, 'you\\'re somewhat dumb')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
